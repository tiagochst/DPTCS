% -%-%-%-%-%-%-%-%-%-%-%-%-%-%-%
% INFMDI 348  
% Date:09/06/2012 
% Paris,France  
% Groupe: 
% - Tiago CHEDRAOUI SILVA 
% - Livia RIBEIRO 
% - Anthony CLERBOUT
% -%-%-%-%-%-%-%-%-%-%-%-%-%-%-%

\documentclass[a4paper,11pt]{article}

\usepackage[english,listings,algo]{tcs}

% Cover %
\def \ttprofname{Mauro Sozio} % teachers name
\def \ttabrv{INFMDI348} % abbreviation of names class
\def \ttabrvxt{} % period
\def \mytitle{Project on Data Mining} % Big title
\def \mysubtitle{Clustering} % subtitle
\def \ttauthi{Tiago CHEDRAUOI SILVA} % author's name
\def \ttdate{June 22, 2012} % date

\begin{document}
\titleTMB 
\newpage
\tableofcontents
\listoffigures
\newpage

\section{Introduction}
The main goal of this project is to clustering a collection of documents so that documents dealing with a same topic belong to a same cluster.
To this purpose, we chose to use the k-means algorithm.

\section{Preprocessing}
 As  input for  the  clustering we  have  1000 documents  that  were taken  from
 blogs. Some steps to remove noisy are apllied: 
1. Remove non-alphabetic characters
2. Remove white spaces
3. Make every word lower case
4. Remove stopwords
5. Remove Words with extremely low or high frequency
6. Normalize the matrix


%\begin{enumerate}
%\item Remove non-alphabetic characters
%\item Remove white spaces
%\item Make every word lower case
%\item Remove stopwords
%\item Remove Words with extremely low or high frequency
%\item Normalize the matrix
%\end{enumerate}

\begin{figure}[h!]
  \begin{centering}
    \includegraphics[trim=0 0 0 50,scale=0.5]{../graphs/q1/Histogram}
    \par\end{centering}
  \caption{Histogram word frequency in a colection of documents}
  \label{fig:jacobi-conv}
\end{figure}


\section{Clustering}

%\begin{multicols}{2}
%  \lstinputlisting[title=\textbf{Kmeans:Example of empty cluster}]{../kmeans.R}
%\end{multicols}

%Q3
We executed  k-means (k =  3) on  our corpus 20  times, each one  with different
initial centers. The best result is showed in table \ref{tab:kmens3}:
\begin{table}[H]
  \caption{Clustering Results}
  \label{tab:kmens3}

  \begin{center}
    \begin{tabular}{|l|l|}
      \hline
      \multicolumn{2}{|c|}{K-means (k = 3)}\\
      \hline
      \hline
      Size Group 1& 389 \\
      Size Group 2& 224 \\
      Size Group 3& 508\\
      \hline
      SSE & 999.9103 \\
      \hline
    \end{tabular}
  \end{center}
\end{table}


To show an exemple  of empty cluster, we have chosen some  points and centers so
that in an iteraction,  a recalculated center is more far from  one point in its
cluster than the center of another cluster. 
If all points of a cluster are closer to other cluster centers, then the next iteration the cluster will be emptied.

The figure \ref{fig:emptya} shows the initial points and centers. After the 
first interaction, we have 3 cluster (blue: 1 point, black: 2 point, red: 4
point) showed in the figure \ref{fig:emptyb}. Finaly the figure \ref{fig:emptyc}
shows that after the recalculation of the
centers, we got an empty cluster (blue: 2 point, black: empty, red: 5 points)

\begin{figure}[ht!]
  \begin{centering}
    \subfigure[Initial Points]{
      \includegraphics[trim = 0 0 50 50,scale=0.35]{../graphs/q2/InitialPoints}
      \label{fig:emptya}
    }
    \subfigure[Iteraction 1]{
      \includegraphics[trim = 0 0 50 50,scale=0.35]{../graphs/q2/0Empty}
      \label{fig:emptyb}
    }
    \subfigure[Iteraction 2]{
      \includegraphics[trim=0 0 50 50,scale=0.35]{../graphs/q2/1Empty}
      \label{fig:emptyc}
    }
  \end{centering}
  \caption{Kmeans: Example of empty cluster}
  \label{fig:empty}

\end{figure}
  
%\begin{figure}[hb!]
%  \begin{centering}
%    \includegraphics[trim=0 0 0 100,scale=0.5]{../graphs/q2/0Empty}
%    \par\end{centering}
%  \caption{Kmeans: Example of empty cluster Iteraction 1}
%  \label{fig:jacobi-conv}
%\end{figure}


%\begin{figure}[h!]
%  \begin{centering}
%    \includegraphics[trim=0 0 0 100,scale=0.5]{../graphs/q2/1Empty}
%    \par\end{centering}
%  \caption{Kmeans: Example of empty cluster Iteraction 2}
%  \label{fig:jacobi-conv}
%\end{figure}

% http://www.stanford.edu/~hastie/Papers/gap.pdf
\subsection{Clustering validation}


Some studies were made to chose the best k of k-means.

The first simple analysis is the calculus of SSE (sum of the squared error
)for differents values of k.
It  is  obvious  that increasing  k  lead  us  to  a  small value  of  SSE  (see
figure \label{fig:kSSE}). So we need to choose the k where we have an elbow. In
figure \label{fig:kSSE} this knee is found in  $k_{good} = 5$



%Hartigan (1975) proposed the statistic:
%\begin{equation*}
%H(k) = \frac{W(k)}{W(k+1)}-1*(n-k-1)
%\end{equation*}
%where  n is  the number  of instances  being clustered  and k  is the  number of
%sets. Hartigan suggests that if H(k)>10 the cluster should be added, if H(k)<10,
%the cluster is not added and the algorithm is stop.

%Applying Hartigan, we got the graph \ref{fig:ka} which gives $k_{good} = 5$

Also, using  the methode  of average silhouette  coefficient wich  combines both
cohesion and separation we got the graph \ref{fig:kb}, which gives $k_{good} = 4$.


%\begin{figure}[h!]
%  \begin{centering}
%    \includegraphics[trim=0 0 0 50,scale=0.5]{../graphs/q4/SSEerror}
%    \par\end{centering}
%  \caption{Kmeans validation}
%  \label{fig:jacobi-conv}
%\end{figure}


\begin{figure}[ht!]
  \begin{centering}
    \subfigure[SSE vs k]{
    \includegraphics[trim=0 0 50 50,scale=0.35]{../graphs/q4/SSEerror}
      \label{fig:kSSE}
    }
 %   \subfigure[Hartigan Calculus]{
 %     \includegraphics[trim = 0 0 50 50,scale=0.35]{../graphs/q4/HT}
 %     \label{fig:ka}
 %   }
    \subfigure[Silhouette  Coefficient]{
      \includegraphics[trim = 0 0 50 50,scale=0.35]{../graphs/q4/SIL}
      \label{fig:kb}
    }
  \end{centering}
  \caption{Kmeans validation}
  \label{fig:val}
\end{figure}


\begin{table}
  \begin{centering}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
      \cline{2-15} 
      \multicolumn{1}{c|}{} & Time & SSE & \multicolumn{4}{c|}{Distrib} & \multicolumn{4}{c|}{Purity} & \multicolumn{4}{c|}{Entropy}\tabularnewline
      \hline 
      Fixe & 0.028 & 987.42 & 367 & 381 & 218 & 155 & 0.59 & 1 & 0.94 & 0.98 & 1.34 & 0 & -0.30 & -0.99\tabularnewline
      \hline 
      Rand (nstart = 1) & 0.044 & 988.28 & 301 & 222 & 434 & 164 & 0.98 & 1 & 0.74 & 1 & 0.11 & 0 & 1.08 & 0\tabularnewline
      \hline 
      Rand (nstart = 5) & 0.273 & 986.35 & 383 & 219 & 422 & 97 & 1 & 0.94 & 0.65 & 0.98 & 0 & 0.30 & 1.23 & 0.08\tabularnewline
      \hline 
      Rand (nstart = 10) & 0.323 & 986.35 & 383 & 97 & 422 & 219 & 1 & 0.99 & 0.65 & 0.94 & 0 & 0.08 & 1.23 & 0.30\tabularnewline
      \hline 
      Rand (nstart = 20) & 0.57 & 985.88 & 77 & 384 & 219 & 441 & 0.99 & 1 & 0.94 & 0.66 & 0.1 & 0 & 0.3 & 1.2\tabularnewline
      \hline 
    \end{tabular}
    \par\end{centering}
  
  \caption{Resultats k-means }
  
\end{table}

\end{document}

